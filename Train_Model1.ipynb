{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_Model1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhTa9vq9ck0D1JuhH4Qhcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeebafatima/Music-Composer/blob/main/Train_Model1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoszZCYVOFmn"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34wjIRBFRX3u",
        "outputId": "1e6ce654-76b5-4933-d6f6-fe6ce96b0120",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm59HpuzRwNR",
        "outputId": "f08b8433-feef-41b0-918c-a305206fba58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Music Composer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Music Composer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF4GODRuvKNb",
        "outputId": "1c434b76-e83c-4137-c3ec-3cdeccd58919",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mCode\u001b[0m/  \u001b[01;34mData\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a_wx6waOYWM"
      },
      "source": [
        "data_directory = \"Data\"\n",
        "data_file = \"Data/Input_Music_Tunes.txt\"\n",
        "char_index_json_file = \"Data/char_to_index.json\"\n",
        "model_weights_location = 'Data/Model1/Model1_Weights/'\n",
        "BATCH_SIZE = 16\n",
        "SEQ_LENGTH = 64"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6vdbWT7OiqU"
      },
      "source": [
        "def read_batches(all_chars, unique_chars):\n",
        "    length = all_chars.shape[0]\n",
        "    batch_chars = int(length / BATCH_SIZE) #129665/16 = 8104 //blocks\n",
        "    \n",
        "    for start in range(0, batch_chars - SEQ_LENGTH, 64):  #(0, 8104, 64)  #it denotes number of batches. It runs everytime when new batch is created. We have a total of 126 batches.\n",
        "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))    #(16, 64)\n",
        "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, unique_chars))   #(16, 64, 86)\n",
        "        for batch_index in range(0, 16):  #it denotes each row in a batch.  \n",
        "            for i in range(0, 64):  #it denotes each column in a batch. Each column represents each character means \n",
        "                #each time-step character in a sequence.\n",
        "                X[batch_index, i] = all_chars[batch_index * batch_chars + start + i]\n",
        "                Y[batch_index, i, all_chars[batch_index * batch_chars + start + i + 1]] = 1 #here we have added '1' because the\n",
        "                #correct label will be the next character in the sequence. So, the next character will be denoted by\n",
        "                #all_chars[batch_index * batch_chars + start + i + 1]\n",
        "        yield X, Y"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0JOebS-4B1j"
      },
      "source": [
        "def built_model(batch_size, seq_length, unique_chars):\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(input_dim = unique_chars, output_dim = 512, batch_input_shape = (batch_size, seq_length))) \n",
        "    \n",
        "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(LSTM(128, return_sequences = True, stateful = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(TimeDistributed(Dense(unique_chars)))\n",
        "\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyymf8pfOtJP"
      },
      "source": [
        "def train_model(data,epochs=1,save_freq=10):\n",
        "    char_to_index = { ch: i for (i, ch) in enumerate(sorted(list(set(data)))) }\n",
        "    print(\"Number of unique characters in our whole tunes train data = {}\".format(len(char_to_index)))\n",
        "    \n",
        "    with open(char_index_json_file, mode = \"w\") as f:\n",
        "        json.dump(char_to_index, f)\n",
        "    \n",
        "    index_to_char = {i: ch for (ch, i) in char_to_index.items()}\n",
        "    unique_chars = len(char_to_index)\n",
        "    \n",
        "    model = built_model(BATCH_SIZE, SEQ_LENGTH, unique_chars)\n",
        "    model.summary()\n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "    \n",
        "    all_characters = np.asarray([char_to_index[c] for c in data], dtype = np.int32)\n",
        "    print(\"Total number of characters = \"+str(all_characters.shape[0])) #129665\n",
        "    \n",
        "    epoch_number, loss, accuracy = [], [], []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
        "        final_epoch_loss, final_epoch_accuracy = 0, 0\n",
        "        epoch_number.append(epoch+1)\n",
        "        \n",
        "        for i, (x, y) in enumerate(read_batches(all_characters, unique_chars)):\n",
        "            final_epoch_loss, final_epoch_accuracy = model.train_on_batch(x, y) #check documentation of train_on_batch here: https://keras.io/models/sequential/\n",
        "            print(\"Batch: {}, Loss: {}, Accuracy: {}\".format(i+1, final_epoch_loss, final_epoch_accuracy))\n",
        "            #here, above we are reading the batches one-by-one and train our model on each batch one-by-one.\n",
        "        loss.append(final_epoch_loss)\n",
        "        accuracy.append(final_epoch_accuracy)\n",
        "        \n",
        "        #saving weights after every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            model.save_weights((model_weights_location+\"Weights_{}.h5\".format(epoch+1)))\n",
        "            print('Saved Weights at epoch {} to file Weights_{}.h5'.format(epoch+1, epoch+1))\n",
        "    \n",
        "    #creating dataframe and record all the losses and accuracies at each epoch\n",
        "    log_frame = pd.DataFrame(columns = [\"Epoch\", \"Loss\", \"Accuracy\"])\n",
        "    log_frame[\"Epoch\"] = epoch_number\n",
        "    log_frame[\"Loss\"] = loss\n",
        "    log_frame[\"Accuracy\"] = accuracy\n",
        "    log_frame.to_csv(\"Data/Model1/log1.csv\", index = False)\n",
        "    \n",
        "    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GomWO0tEPEDH",
        "outputId": "1be96c2a-4032-4b3d-d3d8-fab67e21700a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "file = open(data_file, mode = 'r')\n",
        "data = file.read()\n",
        "file.close()\n",
        "if __name__ == \"__main__\":\n",
        "    train_model(data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique characters in our whole tunes train data = 86\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (16, 64, 512)             44032     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (16, 64, 256)             787456    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (16, 64, 256)             0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (16, 64, 128)             197120    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (16, 64, 128)             0         \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (16, 64, 86)              11094     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (16, 64, 86)              0         \n",
            "=================================================================\n",
            "Total params: 1,039,702\n",
            "Trainable params: 1,039,702\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Total number of characters = 129665\n",
            "Epoch 1/1\n",
            "Batch: 1, Loss: 4.4532246589660645, Accuracy: 0.009765625\n",
            "Batch: 2, Loss: 4.430805206298828, Accuracy: 0.150390625\n",
            "Batch: 3, Loss: 4.399097442626953, Accuracy: 0.1220703125\n",
            "Batch: 4, Loss: 4.306493759155273, Accuracy: 0.1474609375\n",
            "Batch: 5, Loss: 4.094907283782959, Accuracy: 0.1611328125\n",
            "Batch: 6, Loss: 3.891749858856201, Accuracy: 0.1416015625\n",
            "Batch: 7, Loss: 3.8289246559143066, Accuracy: 0.1357421875\n",
            "Batch: 8, Loss: 3.779432535171509, Accuracy: 0.12890625\n",
            "Batch: 9, Loss: 3.7553539276123047, Accuracy: 0.1240234375\n",
            "Batch: 10, Loss: 3.6909079551696777, Accuracy: 0.1181640625\n",
            "Batch: 11, Loss: 3.548147201538086, Accuracy: 0.138671875\n",
            "Batch: 12, Loss: 3.577265501022339, Accuracy: 0.1357421875\n",
            "Batch: 13, Loss: 3.616419792175293, Accuracy: 0.1376953125\n",
            "Batch: 14, Loss: 3.6015782356262207, Accuracy: 0.1455078125\n",
            "Batch: 15, Loss: 3.5915145874023438, Accuracy: 0.138671875\n",
            "Batch: 16, Loss: 3.477471351623535, Accuracy: 0.15625\n",
            "Batch: 17, Loss: 3.4293417930603027, Accuracy: 0.158203125\n",
            "Batch: 18, Loss: 3.3666906356811523, Accuracy: 0.1591796875\n",
            "Batch: 19, Loss: 3.5935583114624023, Accuracy: 0.1259765625\n",
            "Batch: 20, Loss: 3.566269874572754, Accuracy: 0.111328125\n",
            "Batch: 21, Loss: 3.603193759918213, Accuracy: 0.1171875\n",
            "Batch: 22, Loss: 3.559317111968994, Accuracy: 0.1083984375\n",
            "Batch: 23, Loss: 3.3689513206481934, Accuracy: 0.1376953125\n",
            "Batch: 24, Loss: 3.5401134490966797, Accuracy: 0.1162109375\n",
            "Batch: 25, Loss: 3.55924129486084, Accuracy: 0.119140625\n",
            "Batch: 26, Loss: 3.5484561920166016, Accuracy: 0.115234375\n",
            "Batch: 27, Loss: 3.269866466522217, Accuracy: 0.1767578125\n",
            "Batch: 28, Loss: 3.399574041366577, Accuracy: 0.1513671875\n",
            "Batch: 29, Loss: 3.5785844326019287, Accuracy: 0.134765625\n",
            "Batch: 30, Loss: 3.5274484157562256, Accuracy: 0.1318359375\n",
            "Batch: 31, Loss: 3.3942577838897705, Accuracy: 0.154296875\n",
            "Batch: 32, Loss: 3.476111888885498, Accuracy: 0.1376953125\n",
            "Batch: 33, Loss: 3.496704339981079, Accuracy: 0.1357421875\n",
            "Batch: 34, Loss: 3.433136463165283, Accuracy: 0.134765625\n",
            "Batch: 35, Loss: 3.4138569831848145, Accuracy: 0.14453125\n",
            "Batch: 36, Loss: 3.619335412979126, Accuracy: 0.119140625\n",
            "Batch: 37, Loss: 3.503690719604492, Accuracy: 0.1083984375\n",
            "Batch: 38, Loss: 3.476292133331299, Accuracy: 0.115234375\n",
            "Batch: 39, Loss: 3.4538211822509766, Accuracy: 0.1240234375\n",
            "Batch: 40, Loss: 3.456402540206909, Accuracy: 0.115234375\n",
            "Batch: 41, Loss: 3.4872117042541504, Accuracy: 0.1240234375\n",
            "Batch: 42, Loss: 3.331677198410034, Accuracy: 0.1435546875\n",
            "Batch: 43, Loss: 3.4031529426574707, Accuracy: 0.1533203125\n",
            "Batch: 44, Loss: 3.5827574729919434, Accuracy: 0.12109375\n",
            "Batch: 45, Loss: 3.534696102142334, Accuracy: 0.1318359375\n",
            "Batch: 46, Loss: 3.425273895263672, Accuracy: 0.146484375\n",
            "Batch: 47, Loss: 3.378114700317383, Accuracy: 0.1494140625\n",
            "Batch: 48, Loss: 3.4113430976867676, Accuracy: 0.1513671875\n",
            "Batch: 49, Loss: 3.5202255249023438, Accuracy: 0.1201171875\n",
            "Batch: 50, Loss: 3.4251906871795654, Accuracy: 0.1435546875\n",
            "Batch: 51, Loss: 3.3419299125671387, Accuracy: 0.150390625\n",
            "Batch: 52, Loss: 3.244171142578125, Accuracy: 0.1728515625\n",
            "Batch: 53, Loss: 3.4421682357788086, Accuracy: 0.1337890625\n",
            "Batch: 54, Loss: 3.526527166366577, Accuracy: 0.1220703125\n",
            "Batch: 55, Loss: 3.524592876434326, Accuracy: 0.1357421875\n",
            "Batch: 56, Loss: 3.3187129497528076, Accuracy: 0.1279296875\n",
            "Batch: 57, Loss: 3.2322912216186523, Accuracy: 0.1767578125\n",
            "Batch: 58, Loss: 3.4655826091766357, Accuracy: 0.142578125\n",
            "Batch: 59, Loss: 3.5825657844543457, Accuracy: 0.12890625\n",
            "Batch: 60, Loss: 3.373292922973633, Accuracy: 0.158203125\n",
            "Batch: 61, Loss: 3.3784122467041016, Accuracy: 0.154296875\n",
            "Batch: 62, Loss: 3.1241326332092285, Accuracy: 0.18359375\n",
            "Batch: 63, Loss: 3.4606990814208984, Accuracy: 0.1396484375\n",
            "Batch: 64, Loss: 3.4237446784973145, Accuracy: 0.1376953125\n",
            "Batch: 65, Loss: 3.23634672164917, Accuracy: 0.1591796875\n",
            "Batch: 66, Loss: 3.2465434074401855, Accuracy: 0.1572265625\n",
            "Batch: 67, Loss: 3.3493576049804688, Accuracy: 0.1533203125\n",
            "Batch: 68, Loss: 3.314500331878662, Accuracy: 0.15234375\n",
            "Batch: 69, Loss: 3.2310614585876465, Accuracy: 0.1533203125\n",
            "Batch: 70, Loss: 3.0921452045440674, Accuracy: 0.1796875\n",
            "Batch: 71, Loss: 3.337698459625244, Accuracy: 0.162109375\n",
            "Batch: 72, Loss: 3.6289775371551514, Accuracy: 0.13671875\n",
            "Batch: 73, Loss: 3.5089216232299805, Accuracy: 0.150390625\n",
            "Batch: 74, Loss: 3.3462677001953125, Accuracy: 0.1748046875\n",
            "Batch: 75, Loss: 3.3367152214050293, Accuracy: 0.1611328125\n",
            "Batch: 76, Loss: 3.3373801708221436, Accuracy: 0.1689453125\n",
            "Batch: 77, Loss: 3.385970115661621, Accuracy: 0.16015625\n",
            "Batch: 78, Loss: 3.260345935821533, Accuracy: 0.162109375\n",
            "Batch: 79, Loss: 3.2030391693115234, Accuracy: 0.1884765625\n",
            "Batch: 80, Loss: 3.0170538425445557, Accuracy: 0.212890625\n",
            "Batch: 81, Loss: 3.262603759765625, Accuracy: 0.1904296875\n",
            "Batch: 82, Loss: 3.344564437866211, Accuracy: 0.16796875\n",
            "Batch: 83, Loss: 3.2058000564575195, Accuracy: 0.19140625\n",
            "Batch: 84, Loss: 3.170806407928467, Accuracy: 0.212890625\n",
            "Batch: 85, Loss: 2.940504550933838, Accuracy: 0.232421875\n",
            "Batch: 86, Loss: 3.1772942543029785, Accuracy: 0.197265625\n",
            "Batch: 87, Loss: 3.403273105621338, Accuracy: 0.158203125\n",
            "Batch: 88, Loss: 3.1054654121398926, Accuracy: 0.228515625\n",
            "Batch: 89, Loss: 3.2252016067504883, Accuracy: 0.201171875\n",
            "Batch: 90, Loss: 3.1883883476257324, Accuracy: 0.2236328125\n",
            "Batch: 91, Loss: 2.940260410308838, Accuracy: 0.2509765625\n",
            "Batch: 92, Loss: 3.1268959045410156, Accuracy: 0.21484375\n",
            "Batch: 93, Loss: 3.1582093238830566, Accuracy: 0.193359375\n",
            "Batch: 94, Loss: 2.978891134262085, Accuracy: 0.2294921875\n",
            "Batch: 95, Loss: 2.8851876258850098, Accuracy: 0.2509765625\n",
            "Batch: 96, Loss: 2.8399157524108887, Accuracy: 0.2421875\n",
            "Batch: 97, Loss: 3.1697325706481934, Accuracy: 0.19140625\n",
            "Batch: 98, Loss: 3.127439498901367, Accuracy: 0.2080078125\n",
            "Batch: 99, Loss: 2.838313102722168, Accuracy: 0.26171875\n",
            "Batch: 100, Loss: 2.8498342037200928, Accuracy: 0.2529296875\n",
            "Batch: 101, Loss: 3.040087938308716, Accuracy: 0.208984375\n",
            "Batch: 102, Loss: 3.1381964683532715, Accuracy: 0.212890625\n",
            "Batch: 103, Loss: 3.0361344814300537, Accuracy: 0.2333984375\n",
            "Batch: 104, Loss: 2.929013252258301, Accuracy: 0.2353515625\n",
            "Batch: 105, Loss: 2.8511624336242676, Accuracy: 0.279296875\n",
            "Batch: 106, Loss: 2.908874034881592, Accuracy: 0.255859375\n",
            "Batch: 107, Loss: 2.91715145111084, Accuracy: 0.251953125\n",
            "Batch: 108, Loss: 2.8366684913635254, Accuracy: 0.275390625\n",
            "Batch: 109, Loss: 2.7639970779418945, Accuracy: 0.287109375\n",
            "Batch: 110, Loss: 2.939486265182495, Accuracy: 0.232421875\n",
            "Batch: 111, Loss: 2.943997859954834, Accuracy: 0.232421875\n",
            "Batch: 112, Loss: 2.943521499633789, Accuracy: 0.2294921875\n",
            "Batch: 113, Loss: 2.844442844390869, Accuracy: 0.259765625\n",
            "Batch: 114, Loss: 3.032306432723999, Accuracy: 0.2236328125\n",
            "Batch: 115, Loss: 2.883897304534912, Accuracy: 0.240234375\n",
            "Batch: 116, Loss: 2.8637452125549316, Accuracy: 0.236328125\n",
            "Batch: 117, Loss: 3.105454206466675, Accuracy: 0.2197265625\n",
            "Batch: 118, Loss: 2.8072433471679688, Accuracy: 0.2763671875\n",
            "Batch: 119, Loss: 2.690330743789673, Accuracy: 0.2666015625\n",
            "Batch: 120, Loss: 2.7677812576293945, Accuracy: 0.2822265625\n",
            "Batch: 121, Loss: 2.842215061187744, Accuracy: 0.267578125\n",
            "Batch: 122, Loss: 3.035489559173584, Accuracy: 0.248046875\n",
            "Batch: 123, Loss: 2.9471330642700195, Accuracy: 0.2392578125\n",
            "Batch: 124, Loss: 2.774991273880005, Accuracy: 0.2607421875\n",
            "Batch: 125, Loss: 2.732435703277588, Accuracy: 0.2529296875\n",
            "Batch: 126, Loss: 2.9263737201690674, Accuracy: 0.2255859375\n",
            "Saved Weights at epoch 1 to file Weights_1.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bADqo5P-aVG",
        "outputId": "e4f466d0-7a93-405c-8dfa-65f2c150df93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "log = pd.read_csv(\"Data/Model1/log1.csv\")\n",
        "log"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.710444</td>\n",
              "      <td>0.275391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.943644</td>\n",
              "      <td>0.466797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.651903</td>\n",
              "      <td>0.532227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.557028</td>\n",
              "      <td>0.549805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.509097</td>\n",
              "      <td>0.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>96</td>\n",
              "      <td>0.461089</td>\n",
              "      <td>0.844727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>97</td>\n",
              "      <td>0.473274</td>\n",
              "      <td>0.837891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>98</td>\n",
              "      <td>0.474931</td>\n",
              "      <td>0.844727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>99</td>\n",
              "      <td>0.485827</td>\n",
              "      <td>0.849609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>100</td>\n",
              "      <td>0.454271</td>\n",
              "      <td>0.838867</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Epoch      Loss  Accuracy\n",
              "0       1  2.710444  0.275391\n",
              "1       2  1.943644  0.466797\n",
              "2       3  1.651903  0.532227\n",
              "3       4  1.557028  0.549805\n",
              "4       5  1.509097  0.562500\n",
              "..    ...       ...       ...\n",
              "95     96  0.461089  0.844727\n",
              "96     97  0.473274  0.837891\n",
              "97     98  0.474931  0.844727\n",
              "98     99  0.485827  0.849609\n",
              "99    100  0.454271  0.838867\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}