{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOKi3aMBA/WpBY6qls1urIY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeebafatima/Music-Composer/blob/main/Train_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoszZCYVOFmn"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34wjIRBFRX3u",
        "outputId": "7ca2c2d7-ee97-4844-b54b-e10c6040bacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9bgEAKKTpBb",
        "outputId": "3114b39a-ca80-4614-82ca-0d71af9917a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mCode\u001b[0m/  \u001b[01;34mData\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s84MvrC4WvM-"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a_wx6waOYWM"
      },
      "source": [
        "data_directory = \"Data\"\n",
        "data_file = \"/Input_Music_Tunes.txt\"\n",
        "char_index_json_file = \"/char_to_index.json\"\n",
        "model_weights_location = 'Data/Model_Weights/'\n",
        "BATCH_SIZE = 16\n",
        "SEQ_LENGTH = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6vdbWT7OiqU"
      },
      "source": [
        "def read_batches(all_chars, unique_chars):\n",
        "    length = all_chars.shape[0]\n",
        "    batch_chars = int(length / BATCH_SIZE) #155222/16 = 9701\n",
        "    \n",
        "    for start in range(0, batch_chars - SEQ_LENGTH, 64):  #(0, 9637, 64)  #it denotes number of batches. It runs everytime when\n",
        "        #new batch is created. We have a total of 151 batches.\n",
        "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))    #(16, 64)\n",
        "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, unique_chars))   #(16, 64, 87)\n",
        "        for batch_index in range(0, 16):  #it denotes each row in a batch.  \n",
        "            for i in range(0, 64):  #it denotes each column in a batch. Each column represents each character means \n",
        "                #each time-step character in a sequence.\n",
        "                X[batch_index, i] = all_chars[batch_index * batch_chars + start + i]\n",
        "                Y[batch_index, i, all_chars[batch_index * batch_chars + start + i + 1]] = 1 #here we have added '1' because the\n",
        "                #correct label will be the next character in the sequence. So, the next character will be denoted by\n",
        "                #all_chars[batch_index * batch_chars + start + i + 1]\n",
        "        yield X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aixzUYHOna_"
      },
      "source": [
        "def built_model(batch_size, seq_length, unique_chars):\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(input_dim = unique_chars, output_dim = 512, batch_input_shape = (batch_size, seq_length))) \n",
        "    \n",
        "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(LSTM(128, return_sequences = True, stateful = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(TimeDistributed(Dense(unique_chars)))\n",
        "\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyymf8pfOtJP"
      },
      "source": [
        "def train_model(data,epochs=1,save_freq=10):\n",
        "    char_to_index = { ch: i for (i, ch) in enumerate(sorted(list(set(data)))) }\n",
        "    print(\"Number of unique characters in our whole tunes train data = {}\".format(len(char_to_index)))\n",
        "    \n",
        "    with open(data_directory+char_index_json_file, mode = \"w\") as f:\n",
        "        json.dump(char_to_index, f)\n",
        "    \n",
        "    index_to_char = {i: ch for (ch, i) in char_to_index.items()}\n",
        "    unique_chars = len(char_to_index)\n",
        "    \n",
        "    model = built_model(BATCH_SIZE, SEQ_LENGTH, unique_chars)\n",
        "    model.summary()\n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "    \n",
        "    all_characters = np.asarray([char_to_index[c] for c in data], dtype = np.int32)\n",
        "    print(\"Total number of characters = \"+str(all_characters.shape[0])) #155222\n",
        "    \n",
        "    epoch_number, loss, accuracy = [], [], []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
        "        final_epoch_loss, final_epoch_accuracy = 0, 0\n",
        "        epoch_number.append(epoch+1)\n",
        "        \n",
        "        for i, (x, y) in enumerate(read_batches(all_characters, unique_chars)):\n",
        "            final_epoch_loss, final_epoch_accuracy = model.train_on_batch(x, y) #check documentation of train_on_batch here: https://keras.io/models/sequential/\n",
        "            print(\"Batch: {}, Loss: {}, Accuracy: {}\".format(i+1, final_epoch_loss, final_epoch_accuracy))\n",
        "            #here, above we are reading the batches one-by-one and train our model on each batch one-by-one.\n",
        "        loss.append(final_epoch_loss)\n",
        "        accuracy.append(final_epoch_accuracy)\n",
        "        \n",
        "        #saving weights after every 10 epochs\n",
        "      \n",
        "        model.save_weights((model_weights_location+\"Weights_{}.h5\".format(epoch+1)))\n",
        "        print('Saved Weights at epoch {} to file Weights_{}.h5'.format(epoch+1, epoch+1))\n",
        "    \n",
        "    #creating dataframe and record all the losses and accuracies at each epoch\n",
        "    log_frame = pd.DataFrame(columns = [\"Epoch\", \"Loss\", \"Accuracy\"])\n",
        "    log_frame[\"Epoch\"] = epoch_number\n",
        "    log_frame[\"Loss\"] = loss\n",
        "    log_frame[\"Accuracy\"] = accuracy\n",
        "    log_frame.to_csv(\"../Data/log.csv\", index = False)\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GomWO0tEPEDH",
        "outputId": "5eb0fb66-dac5-4683-c5ab-c2d97dcfa717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "file = open(data_directory+data_file, mode = 'r')\n",
        "data = file.read()\n",
        "file.close()\n",
        "if __name__ == \"__main__\":\n",
        "    train_model(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique characters in our whole tunes train data = 86\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (16, 64, 512)             44032     \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (16, 64, 256)             787456    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (16, 64, 256)             0         \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (16, 64, 128)             197120    \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (16, 64, 128)             0         \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (16, 64, 86)              11094     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (16, 64, 86)              0         \n",
            "=================================================================\n",
            "Total params: 1,039,702\n",
            "Trainable params: 1,039,702\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Total number of characters = 129665\n",
            "Epoch 1/1\n",
            "Batch: 1, Loss: 4.45595645904541, Accuracy: 0.001953125\n",
            "Batch: 2, Loss: 4.433083534240723, Accuracy: 0.142578125\n",
            "Batch: 3, Loss: 4.408946990966797, Accuracy: 0.1298828125\n",
            "Batch: 4, Loss: 4.3454270362854, Accuracy: 0.1513671875\n",
            "Batch: 5, Loss: 4.184183120727539, Accuracy: 0.158203125\n",
            "Batch: 6, Loss: 4.003355026245117, Accuracy: 0.1435546875\n",
            "Batch: 7, Loss: 3.883070230484009, Accuracy: 0.138671875\n",
            "Batch: 8, Loss: 3.795064926147461, Accuracy: 0.1328125\n",
            "Batch: 9, Loss: 3.816577672958374, Accuracy: 0.126953125\n",
            "Batch: 10, Loss: 3.7430777549743652, Accuracy: 0.109375\n",
            "Batch: 11, Loss: 3.5788259506225586, Accuracy: 0.1171875\n",
            "Batch: 12, Loss: 3.611117124557495, Accuracy: 0.1142578125\n",
            "Batch: 13, Loss: 3.6785202026367188, Accuracy: 0.1201171875\n",
            "Batch: 14, Loss: 3.6153485774993896, Accuracy: 0.1474609375\n",
            "Batch: 15, Loss: 3.6371068954467773, Accuracy: 0.125\n",
            "Batch: 16, Loss: 3.4449009895324707, Accuracy: 0.1494140625\n",
            "Batch: 17, Loss: 3.401336431503296, Accuracy: 0.1591796875\n",
            "Batch: 18, Loss: 3.3644871711730957, Accuracy: 0.16015625\n",
            "Batch: 19, Loss: 3.603635787963867, Accuracy: 0.126953125\n",
            "Batch: 20, Loss: 3.573000192642212, Accuracy: 0.1201171875\n",
            "Batch: 21, Loss: 3.605292320251465, Accuracy: 0.1220703125\n",
            "Batch: 22, Loss: 3.5564565658569336, Accuracy: 0.1240234375\n",
            "Batch: 23, Loss: 3.383577823638916, Accuracy: 0.150390625\n",
            "Batch: 24, Loss: 3.5200483798980713, Accuracy: 0.1328125\n",
            "Batch: 25, Loss: 3.5379087924957275, Accuracy: 0.1279296875\n",
            "Batch: 26, Loss: 3.5538456439971924, Accuracy: 0.1201171875\n",
            "Batch: 27, Loss: 3.2870559692382812, Accuracy: 0.16015625\n",
            "Batch: 28, Loss: 3.3978970050811768, Accuracy: 0.1474609375\n",
            "Batch: 29, Loss: 3.567800521850586, Accuracy: 0.12890625\n",
            "Batch: 30, Loss: 3.547715663909912, Accuracy: 0.126953125\n",
            "Batch: 31, Loss: 3.3915765285491943, Accuracy: 0.1552734375\n",
            "Batch: 32, Loss: 3.4468307495117188, Accuracy: 0.13671875\n",
            "Batch: 33, Loss: 3.528290271759033, Accuracy: 0.13671875\n",
            "Batch: 34, Loss: 3.4105124473571777, Accuracy: 0.14453125\n",
            "Batch: 35, Loss: 3.397646903991699, Accuracy: 0.1337890625\n",
            "Batch: 36, Loss: 3.6087236404418945, Accuracy: 0.1162109375\n",
            "Batch: 37, Loss: 3.5114216804504395, Accuracy: 0.1123046875\n",
            "Batch: 38, Loss: 3.47690749168396, Accuracy: 0.1025390625\n",
            "Batch: 39, Loss: 3.434055805206299, Accuracy: 0.1298828125\n",
            "Batch: 40, Loss: 3.4501149654388428, Accuracy: 0.1259765625\n",
            "Batch: 41, Loss: 3.4816184043884277, Accuracy: 0.1181640625\n",
            "Batch: 42, Loss: 3.2999918460845947, Accuracy: 0.162109375\n",
            "Batch: 43, Loss: 3.3881258964538574, Accuracy: 0.1455078125\n",
            "Batch: 44, Loss: 3.547518491744995, Accuracy: 0.12890625\n",
            "Batch: 45, Loss: 3.4897499084472656, Accuracy: 0.1357421875\n",
            "Batch: 46, Loss: 3.3564791679382324, Accuracy: 0.150390625\n",
            "Batch: 47, Loss: 3.358895778656006, Accuracy: 0.154296875\n",
            "Batch: 48, Loss: 3.3367347717285156, Accuracy: 0.1552734375\n",
            "Batch: 49, Loss: 3.463761806488037, Accuracy: 0.130859375\n",
            "Batch: 50, Loss: 3.350367546081543, Accuracy: 0.150390625\n",
            "Batch: 51, Loss: 3.295541763305664, Accuracy: 0.150390625\n",
            "Batch: 52, Loss: 3.2193031311035156, Accuracy: 0.169921875\n",
            "Batch: 53, Loss: 3.4904725551605225, Accuracy: 0.1337890625\n",
            "Batch: 54, Loss: 4.0028486251831055, Accuracy: 0.103515625\n",
            "Batch: 55, Loss: 4.074767112731934, Accuracy: 0.0927734375\n",
            "Batch: 56, Loss: 3.794736862182617, Accuracy: 0.1044921875\n",
            "Batch: 57, Loss: 3.6864185333251953, Accuracy: 0.1298828125\n",
            "Batch: 58, Loss: 3.66878080368042, Accuracy: 0.107421875\n",
            "Batch: 59, Loss: 3.748511791229248, Accuracy: 0.1015625\n",
            "Batch: 60, Loss: 3.6470932960510254, Accuracy: 0.0966796875\n",
            "Batch: 61, Loss: 3.6266674995422363, Accuracy: 0.109375\n",
            "Batch: 62, Loss: 3.3646726608276367, Accuracy: 0.14453125\n",
            "Batch: 63, Loss: 3.5970876216888428, Accuracy: 0.1259765625\n",
            "Batch: 64, Loss: 3.5901947021484375, Accuracy: 0.1142578125\n",
            "Batch: 65, Loss: 3.4740512371063232, Accuracy: 0.140625\n",
            "Batch: 66, Loss: 3.459758996963501, Accuracy: 0.134765625\n",
            "Batch: 67, Loss: 3.5108156204223633, Accuracy: 0.1357421875\n",
            "Batch: 68, Loss: 3.4872875213623047, Accuracy: 0.1240234375\n",
            "Batch: 69, Loss: 3.4059689044952393, Accuracy: 0.1435546875\n",
            "Batch: 70, Loss: 3.2745201587677, Accuracy: 0.16796875\n",
            "Batch: 71, Loss: 3.4429478645324707, Accuracy: 0.1484375\n",
            "Batch: 72, Loss: 3.508375406265259, Accuracy: 0.1416015625\n",
            "Batch: 73, Loss: 3.3628804683685303, Accuracy: 0.162109375\n",
            "Batch: 74, Loss: 3.233509063720703, Accuracy: 0.173828125\n",
            "Batch: 75, Loss: 3.2150323390960693, Accuracy: 0.1669921875\n",
            "Batch: 76, Loss: 3.3368611335754395, Accuracy: 0.16796875\n",
            "Batch: 77, Loss: 3.3223090171813965, Accuracy: 0.1748046875\n",
            "Batch: 78, Loss: 3.2101831436157227, Accuracy: 0.166015625\n",
            "Batch: 79, Loss: 3.2354207038879395, Accuracy: 0.1611328125\n",
            "Batch: 80, Loss: 3.0979902744293213, Accuracy: 0.185546875\n",
            "Batch: 81, Loss: 3.3194708824157715, Accuracy: 0.1650390625\n",
            "Batch: 82, Loss: 3.372934341430664, Accuracy: 0.173828125\n",
            "Batch: 83, Loss: 3.231407642364502, Accuracy: 0.17578125\n",
            "Batch: 84, Loss: 3.1940603256225586, Accuracy: 0.18359375\n",
            "Batch: 85, Loss: 3.0208983421325684, Accuracy: 0.208984375\n",
            "Batch: 86, Loss: 3.3381223678588867, Accuracy: 0.1572265625\n",
            "Batch: 87, Loss: 3.4485726356506348, Accuracy: 0.1396484375\n",
            "Batch: 88, Loss: 3.1366214752197266, Accuracy: 0.2001953125\n",
            "Batch: 89, Loss: 3.283846139907837, Accuracy: 0.177734375\n",
            "Batch: 90, Loss: 3.194831132888794, Accuracy: 0.1904296875\n",
            "Batch: 91, Loss: 3.003401279449463, Accuracy: 0.20703125\n",
            "Batch: 92, Loss: 3.1989035606384277, Accuracy: 0.1923828125\n",
            "Batch: 93, Loss: 3.2498810291290283, Accuracy: 0.173828125\n",
            "Batch: 94, Loss: 3.041276454925537, Accuracy: 0.2109375\n",
            "Batch: 95, Loss: 2.925536870956421, Accuracy: 0.2294921875\n",
            "Batch: 96, Loss: 2.9142653942108154, Accuracy: 0.224609375\n",
            "Batch: 97, Loss: 3.286787748336792, Accuracy: 0.177734375\n",
            "Batch: 98, Loss: 3.1804356575012207, Accuracy: 0.2001953125\n",
            "Batch: 99, Loss: 2.924628734588623, Accuracy: 0.23828125\n",
            "Batch: 100, Loss: 2.9256317615509033, Accuracy: 0.2236328125\n",
            "Batch: 101, Loss: 3.11600399017334, Accuracy: 0.20703125\n",
            "Batch: 102, Loss: 3.217377185821533, Accuracy: 0.1806640625\n",
            "Batch: 103, Loss: 3.105680227279663, Accuracy: 0.2177734375\n",
            "Batch: 104, Loss: 3.033608913421631, Accuracy: 0.21875\n",
            "Batch: 105, Loss: 2.948068618774414, Accuracy: 0.248046875\n",
            "Batch: 106, Loss: 3.0205812454223633, Accuracy: 0.224609375\n",
            "Batch: 107, Loss: 3.012443780899048, Accuracy: 0.2265625\n",
            "Batch: 108, Loss: 2.9580016136169434, Accuracy: 0.25390625\n",
            "Batch: 109, Loss: 2.846810817718506, Accuracy: 0.251953125\n",
            "Batch: 110, Loss: 3.024871349334717, Accuracy: 0.2216796875\n",
            "Batch: 111, Loss: 2.9966835975646973, Accuracy: 0.236328125\n",
            "Batch: 112, Loss: 2.9981462955474854, Accuracy: 0.22265625\n",
            "Batch: 113, Loss: 2.9555978775024414, Accuracy: 0.224609375\n",
            "Batch: 114, Loss: 3.1002604961395264, Accuracy: 0.2294921875\n",
            "Batch: 115, Loss: 2.9821388721466064, Accuracy: 0.2275390625\n",
            "Batch: 116, Loss: 2.9296493530273438, Accuracy: 0.240234375\n",
            "Batch: 117, Loss: 3.1630747318267822, Accuracy: 0.1923828125\n",
            "Batch: 118, Loss: 2.9237112998962402, Accuracy: 0.2333984375\n",
            "Batch: 119, Loss: 2.742669105529785, Accuracy: 0.2587890625\n",
            "Batch: 120, Loss: 2.8803367614746094, Accuracy: 0.2392578125\n",
            "Batch: 121, Loss: 2.8940634727478027, Accuracy: 0.255859375\n",
            "Batch: 122, Loss: 3.0709726810455322, Accuracy: 0.208984375\n",
            "Batch: 123, Loss: 3.017420768737793, Accuracy: 0.20703125\n",
            "Batch: 124, Loss: 2.8465030193328857, Accuracy: 0.232421875\n",
            "Batch: 125, Loss: 2.8347835540771484, Accuracy: 0.2412109375\n",
            "Batch: 126, Loss: 2.9832842350006104, Accuracy: 0.2197265625\n",
            "Saved Weights at epoch 1 to file Weights_1.h5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}